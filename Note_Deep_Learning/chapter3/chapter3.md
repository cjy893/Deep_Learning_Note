# 神经网络表示
## 只有一个隐藏层的神经网络
神经网络通常有多个隐藏层,但为了更加直观简洁地解释神经网络,所以使用只有一个隐藏层的神经网络来进行讲解.

神经网络的第一层通常是样本的特征参数,我们称之为**输入层**,而中间一层的圆圈我们称之为**神经网络的隐藏层**,而最后一层通常只有一个节点,我们称之为**输出层**
****
**隐藏层**的含义就是我们看不到这些中间节点的真正数值\
我们可以看到训练集的输入值,也可以看到输出值,但是无法看到中间节点的数值,所以中间的节点层被称为**隐藏层**
## 神经网络中一些俗成的约定
输入特征的值还有另外一种表示方式,我们表示为
$$a^{[0]} = x$$
$a$意为**激活**,表示网络中不同层的值,然后会被传递给后面的层,从而产生新的激活值$a$,所以我们将输入层的值称为$a^{[1]}$

隐藏层也会产生激活值,我们称之为$a^{[1]}$,而隐藏层的第一个节点产生的激活值,我们称之为$a^{[1]}_1$,与此相似的,第二个节点称为$a^{[1]}_2$,以此类推.\
$a^{[1]}$在这里是一个n维列向量(n是该层节点的个数)
****
在输出层会产生$a^{[2]}$,也就是logistic回归中的$\hat{y}$,这又是为什么这个值在之前的计算中又被命名为$a$

但是之前的计算中我们只揭示了这一层输出,在神经网络中,将会使用 带方括号上标的值来明确表明这个值来自于哪一层
****
在神经网络中,我们通常将**输入层**称为**第零层**,所以有一个隐藏层的神经网络,我们通常称之为**双层神经网络**,在之前的学习中所接触的logistic回归是一个**单层神经网络**,因为我们通常不把输入层看做一个标准的层

隐藏层和输入层都是有相关的参数的,比如说,在第一个隐藏层中,我们具有参数$w^{[1]},b^{[1]}$,来表示这是第一层的参数\
并且在其中,$w^{[1]}$是一个(4,3)的矩阵,**4表示有四个节点,3表示有3个输入特征**,而$b^{[1]}$则是一个(4,1)的矩阵\
类似地,输出层也有相关的参数,并且$w^{[2]}$是一个(1,3)的矩阵,而$b^{[2]}$则是(1,1)
# 计算神经网络的输出
## 神经网络的计算过程
在神经网络中,以logistic回归为例,一个节点中通常有两个计算步骤,首先按步骤计算出$z$,然后第二步计算$sigmoid(z)$激活函数.\
所以,神经网络只不过是重复计算这些步骤很多次,得到的$a$则作为下一层的特征参数

## 神经网络中计算的向量化
根据以上描述,我们就需要在一层中进行多次这样的计算,使用for循环仍然会十分低效,所以我们仍然需要将这一过程**向量化**
所以我们将行向量$\vec{w}^T$纵向堆叠,得到一个矩阵$W^T$,将这个矩阵与列向量$\vec{x}$进行矩阵乘法,得到
$$\vec{z}=W^{T}\vec{x}+\vec{b}$$

接下来,就如同之前讲的,利用python的广播对$\vec{z}$进行运算,得到$\vec{a}$
## 自己的小总结
实际上,神经网络的正向传播所计算出的激活值,需要保存到反向传播中以计算导数,更新$w,b$的参数,而再次的正向传播又得出新的激活值,以此不断优化,最终得出最合适的参数和输出
# 神经网络中多个样本计算的向量化
