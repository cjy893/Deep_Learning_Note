# logistic回归中的梯度下降法
## 计算流程
根据公式
$$z=w^T\vec{x}$$
现在假设样本只有两个参数$x_1,x_2$\
为了计算$z$,将参数带入,则有$z=w_1x_1+w_2x_2+b$\
那么根据下一步公式,可以得到$\hat{y}=a=\sigma(z)$\
最后计算得出$L(a,y)=-(y\ln{a}+(1-y)\ln{(1-a)})$\
****
在**logistic回归**中，我们需要做的就是变换参数$w,b$的值,来最小化损失函数$J(w,b)$

现在,我们需要先计算出损失函数的导数,并且很容易求出
$$\frac{dL(a,y)}{da}=-\frac{y}{a}+\frac{1-y}{1-a}$$
并且也可以计算
$$\frac{dL(a,y)}{dz}=\frac{dL(a,y)}{da}.\frac{da}{dz}=(-\frac{y}{a}+\frac{1-y}{1-a}).a(1-a)=a-y$$
最后,我们需要计算关于$w,b$的导数
$$\frac{dL}{dw_1}=\frac{dL}{dz}.\frac{dz}{dw_1}=x_1(a-y)$$
同理
$$\frac{dL}{dw_2}=x_2(a-y)$$
$$\frac{dL}{db}=a-y$$
因此可以得到迭代步骤
$$w_1^{'}=w_1-\alpha\frac{dL}{dw_1}$$
$$w_2^{'}=w_2-\alpha\frac{dL}{dw_2}$$
$$b^{'}=b-\alpha\frac{dL}{db}$$
***这只是针对一个样本的计算，实际上，训练模型应当使用有m个训练样本的整个训练集***